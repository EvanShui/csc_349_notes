{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Asymptotic Notations and Basic Efficiency Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asymptotic analysis of an algorithm refers to defining the mathematical boundation/framing of its run-time performance. Using asymptotic analysis, we can very well conclude the best case, average case, and worst case scenario of an algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To compare orders of growth, we have three notations:\n",
    "    * O = big oh\n",
    "    * $\\Omega$ = big omega\n",
    "    * $\\Theta$ = big theta\n",
    "* Other notations:\n",
    "    * t(n) and g(n) are non-negative functions, doesn't take in negative integers, where $n \\in \\mathbb{N}$\n",
    "    * t(n) will be an algorithm's running time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informal Introduction\n",
    "* **Big oh**: O(g(n)) is the set of all functions with a lower or same order of growth as g(n) (to within a constant multiple as n goes to infinity) <- basically means if equal, if n is a constant multiple then g(n) satisfies as being a subset of big oh.\n",
    "    * E.x. $n \\in O(n^2)$ where n has a lower order of growth (linear)\n",
    "    * $100n + 5 \\in O(n^2)$ where (100n + 5) has a lower order of growth (linear)\n",
    "    * $\\frac{1}{2}n(n-1) \\in O(n^2)$ where (1/2)(n)(n-1) has an equal order of growth (second power)\n",
    "* Examples that are not true\n",
    "    * $n^3 \\notin O(n^2)$\n",
    "    * $0.00001n^3 \\notin O(n^2)$\n",
    "    * $n^4 + n^2 + 1 \\notin O(n^2)$\n",
    "    * All of the examples above have a higher order of growth compared to O(n^2)\n",
    "* From this we can say that if the order of the given function is equal or less than the order of big oh, then said function is a subset of big oh. If not, then said function is not a subset of big oh.\n",
    "* **Omega**: $\\Omega(g(n))$ stands for the set of al functions with a higher or same order of growth as g(n) \n",
    "    * $n^3 \\in \\Omega({n^2})$\n",
    "    * $\\frac{1}{2}(n)(n-1) \\in \\Omega(n^2)$\n",
    "    * but $100n + 5 \\notin \\Omega(n^2)$\n",
    "* **Theta**: $\\Theta(g(n))$ stands for the set of all functions that have the same order of growth as g(n).\n",
    "    * We can say that $\\Theta(g(n)) \\in ax^2 + bx + c$ where $ a \\in \\infty$'\n",
    "* We can only comment on the order of growth for a function that models the number of basic operations.\n",
    "    * E.X. for a function that is O(n^2), we can say that the number of observations grows at a rate similar to n^2 as n increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O - notation\n",
    "* $t(n) \\in O(g(n))$ is true if there exists c where c > 0 and $n_0$ where $n_0 >= 0$ such that $t(n) \\leq cg(n)$ for all $n >= n_0$\n",
    "* We can say that $100n + 5 \\leq 100n + n$ (for all n >= 5)\n",
    "* That expression equals $101n \\leq 101n^2$\n",
    "* We could also say that $100n + 5 \\leq 100n + 5n$ (for all n >= 5) $= 105n$\n",
    "* The reason why we want to find a $n_0$ is because we are really only looking for large value of n when finding a $g(n)$ that is greater than $t(n)$\n",
    "    * $t(n)$ will return us the number of basic operations for a given n, so if $t(n) \\leq O(g(n))$ for $n \\geq n_0$ then the analysis holds true that $t(n)$ grows at a rate less than or equal to $g(n)$ for those n values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OMEGA - notation\n",
    "* $t(n) \\in O(g(n))$ is true if there exists c where c > 0 and $n_0$ where $n_0 >= 0$ such that $t(n) \\geq cg(n)$ for all $n >= n_0$\n",
    "* Formal proof that $n^3 \\in \\Omega(n^2)$\n",
    "    * $n^3 \\geq n^2$ for all $n \\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THETA - notation\n",
    "* $t(n) \\in O(g(n))$ is true if there exists $c_1$ and $c_2$ where $c1, c2$ > 0 and $n_0$ where $n_0 >= 0$ such that \n",
    "    * $c_2(g(n)) \\leq t(n) \\leq c_1(g(n))$ for all $n >= n_0$\n",
    "* Example\n",
    "    * Prove $\\frac{1}{2}n(n-1) \\in \\Theta{n^2}$.\n",
    "    * First, prove the right inequality (right bound)\n",
    "        * $\\frac{1}{2}n(n-1) = \\frac{1}{2}n^2 - \\frac{1}{2}n \\leq \\frac{1}{2}n^2$ for all $n \\geq 0$\n",
    "    * Second, prove the left inequality (left bound)\n",
    "        * $\\frac{1}{2}n(n-1) = \\frac{1}{2}n^2 - \\frac{1}{2}n \\geq \\frac{1}{2}n^2 - \\frac{1}{2}n\\frac{1}{2}n$\n",
    "        * $= -\\frac{1}{2}n \\geq -\\frac{1}{2}n\\frac{1}{2}n$\n",
    "        * $= n \\leq \\frac{1}{2}n^2$\n",
    "        * $= 2 \\leq n = n \\geq 2$\n",
    "    * For this problem, we selected $c_2 = \\frac{1}{4}, c_1 = \\frac{1}{2}, n_0 = 2$\n",
    "    * Will probably have to state that for all future problems, values for c_2, c_1, and n_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Property Involving the Asymptotic Notations\n",
    "* THEOREM: \n",
    "    * If $t_1(n) \\in O(g_1(n))$ and $t_2(n) \\in O(g_2(n))$ then $t_1(n) + t_2(n) \\in O(max({g_1(n), g_2(n)}))$\n",
    "        * This theorem holds true for $\\Omega$ and $\\Theta$\n",
    "* PROOF:\n",
    "    * Extends the proof that if $a_1 \\leq b_1$ and $a_2 \\leq b_2$ then $a_1 + a_2 \\leq 2*max(b_1, b_2)$\n",
    "    * By definition, since $t_1(n) \\in O(g_1(n))$, there exists $c_1 \\gt 0$ and $n_1 \\geq 0$ such that\n",
    "        * $t_1(n) \\leq c_1 * g_1(n)$ for $n \\geq n_1$\n",
    "    * Similarly, since $t_2(n) \\in O(g_2(n))$, there exists $c_2 \\gt 0$ and $n_2 \\geq 0$ such that\n",
    "        * $t_2(n) \\leq c_2 * g_2(n)$ for $n \\geq n_2$\n",
    "    * Let's define $c_3 = {max(c_1, c_2)}$ and consider $n \\geq max({n_1, n_2})$.\n",
    "    * Now, we can combine everything together.\n",
    "        * $t_1(n) + t_2(n) \\leq c_1*g_1(n) + c_2*g_2(n)$\n",
    "        * $t_1(n) + t_2(n) \\leq c_3*g_1(n) + c_3*g_2)n)$\n",
    "        * $t_1(n) + t_2(n) \\leq c_3 * [g_1(n) + g_2(n)]$\n",
    "        * $t_1(n) + t_2(n) \\leq 2 * c_3 * max{(g_1(n), g_2(n))}$\n",
    "        * From this, we can say that $t_1(n) + t_2(n) \\in O(max{(g_1(n), g_2(n))})$\n",
    "            * The reason why the 2 constant multiplier that is on the right side of the inequality on the 4th line doesn't matter when finding big oh is cause all we care about is the rate of growth it increases, 2 increases the value but growth is still mainly dependent on the variable from $max{(g_1(n), g_2(n))}$\n",
    "* This algorithm tells us, in English, that the big oh for two consecutively executed portions of code within an algorithm, execute alg 1 first then execute alg 2 second, is the highest order of the individual big oh's for both functions (or the \"limiting factor\" / least efficient algorithm's big oh).\n",
    "* Example\n",
    "    * There's a two part algorithms, first sorts the array and the second checks for equal values.\n",
    "        * Assume the sorting algorithm takes $\\frac{1}{2}(n)(n-1) \\in O(n^2)$\n",
    "        * The checking is $O(n)$\n",
    "        * Thus, the limiting efficiency is $O(max{(n^2, n)}) -> O(n^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Limits for Comparing Orders of Growth\n",
    "* To compare the orders of growth of two functions, compute the limt of the ratio of the two functions.\n",
    "    * $\\lim{n\\to\\infty} \\frac{t(n)}{g(n)} = 0, c, \\infty$\n",
    "        * if 0, implies that $t(n)$ has a smaller order than $g(n)$\n",
    "        * if c, implies that $t(n)$ has an order equal to $g(n)$\n",
    "        * if $\\infty$, implies that $t(n)$ has an order greater than $g(n)$\n",
    "    * Note that the 0, c case implies that $t(n) \\in O(g(n))$\n",
    "    * the c, $\\infty$, case implies that $t(n) \\in \\Omega(g(n))$\n",
    "    * the c case implies that $t(n) \\in \\Theta(g(n))$\n",
    "* You can also apply L'Hopital's rule to this.\n",
    "    * take the first derivative of both functions and then calculate the ratio as x approaches infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples for limit-based approach\n",
    "* Example 1\n",
    "    * Compare the orders of $\\frac{1}{2}n(n-1)$ and $n^2$\n",
    "        * $\\lim{x\\to\\infty}\\frac{\\frac{1}{2}n(n-1)}{n^2} = \\frac{1}{2}\\lim{x\\to\\infty}\\frac{n^2-n}{n^2} = \\frac{1}{2}\\lim{x\\to\\infty}(1 - \\frac{1}{n}) = \\frac{1}{2}$\n",
    "        * Because the result is a constant, $\\frac{1}{2}$, we can say that $\\frac{1}{2}n(n-1) \\in \\Theta({n^2})$\n",
    "* Example 2\n",
    "    * Compare the orders of $log_2{n}$ and $\\sqrt{n}$\n",
    "        * $\\lim{x\\to\\infty}\\frac{log_2{n}}{\\sqrt{n}} = \\lim{x\\to\\infty}\\frac{(log_2{n})`}{(\\sqrt{n})`} = \\lim{x\\to\\infty}\\frac{(log_2{e})*\\frac{1}{n}}{\\frac{1}{2*\\sqrt{n}}} = 2 * log_2{e} * \\lim{x\\to\\infty}\\frac{\\frac{1}{n}}{\\frac{1}{\\sqrt{n}}} = 2 * log_2{e} * \\lim{x\\to\\infty}\\frac{1}{\\sqrt{n}} = 0$\n",
    "        * Since the result is 0, $log_2{n}$ is $O(\\sqrt{n})$.\n",
    "        * We can use little-oh notation, which states if $t(n) \\in o(g(n))$, then $t(n)$ is strictly less than $g(n)$.\n",
    "* Example 3\n",
    "    * Compare the orders of n! and $2^n$\n",
    "        * $\\lim{x\\to\\infty}\\frac{n!}{2^n} = \\lim{x\\to\\infty}\\frac{\\sqrt{2\\pi*n}(\\frac{n}{e})^n}{2^n} = \\lim{x\\to\\infty}\\sqrt{2\\pi*n}\\frac{n^n}{2^n * e^n} = \\lim{x\\to\\infty}\\sqrt{2\\pi*n}(\\frac{n}{2e})^n = \\infty$\n",
    "    * n! grows faster than $2^n$ we can say that $n! \\in \\Omega(2^n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Efficiency Classes\n",
    "* Concern: Classifying algorithms by their asymptotic efficiency wouldn't be useful, cause the values of the constants, c, n0, are usually unspecified.\n",
    "    * E.x. $10^6 * n^2$ vs. $n^3$. the cubic algorithm outperform the squre until n = $10^6$\n",
    "    * However, multiplicative constants usually don't differ that drastically.\n",
    "    * As a rule, expect an algorithm from a better asymptotic efficiency class to outperform an algorithm from a worse class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
